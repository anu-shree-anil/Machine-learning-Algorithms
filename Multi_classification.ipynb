{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Multi-classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOz7swR4B1nWQH3zUkuwSDT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anu-shree-anil/Machine-learning-Algorithms/blob/main/Multi_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JuvJnyh-bU7H",
        "outputId": "e504e221-5d49-4d9c-89dc-617a4184900c"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import math\n",
        "\n",
        "#loading and preprocessing the data\n",
        "iris=datasets.load_iris()\n",
        "print(iris.DESCR)\n",
        "df=pd.DataFrame(data=np.c_[iris['data'],iris['target']],columns=iris['feature_names']+['target'])\n",
        "X=pd.DataFrame(iris.data)\n",
        "Y=pd.DataFrame(iris.target)\n",
        "print(\"ORIGINAL DATA:\")\n",
        "print(df)\n",
        "print(X,Y)"
      ],
      "execution_count": 243,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".. _iris_dataset:\n",
            "\n",
            "Iris plants dataset\n",
            "--------------------\n",
            "\n",
            "**Data Set Characteristics:**\n",
            "\n",
            "    :Number of Instances: 150 (50 in each of three classes)\n",
            "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
            "    :Attribute Information:\n",
            "        - sepal length in cm\n",
            "        - sepal width in cm\n",
            "        - petal length in cm\n",
            "        - petal width in cm\n",
            "        - class:\n",
            "                - Iris-Setosa\n",
            "                - Iris-Versicolour\n",
            "                - Iris-Virginica\n",
            "                \n",
            "    :Summary Statistics:\n",
            "\n",
            "    ============== ==== ==== ======= ===== ====================\n",
            "                    Min  Max   Mean    SD   Class Correlation\n",
            "    ============== ==== ==== ======= ===== ====================\n",
            "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
            "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
            "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
            "    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
            "    ============== ==== ==== ======= ===== ====================\n",
            "\n",
            "    :Missing Attribute Values: None\n",
            "    :Class Distribution: 33.3% for each of 3 classes.\n",
            "    :Creator: R.A. Fisher\n",
            "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
            "    :Date: July, 1988\n",
            "\n",
            "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
            "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
            "Machine Learning Repository, which has two wrong data points.\n",
            "\n",
            "This is perhaps the best known database to be found in the\n",
            "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
            "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
            "data set contains 3 classes of 50 instances each, where each class refers to a\n",
            "type of iris plant.  One class is linearly separable from the other 2; the\n",
            "latter are NOT linearly separable from each other.\n",
            "\n",
            ".. topic:: References\n",
            "\n",
            "   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
            "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
            "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
            "   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
            "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
            "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
            "     Structure and Classification Rule for Recognition in Partially Exposed\n",
            "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
            "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
            "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
            "     on Information Theory, May 1972, 431-433.\n",
            "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
            "     conceptual clustering system finds 3 classes in the data.\n",
            "   - Many, many more ...\n",
            "ORIGINAL DATA:\n",
            "     sepal length (cm)  sepal width (cm)  ...  petal width (cm)  target\n",
            "0                  5.1               3.5  ...               0.2     0.0\n",
            "1                  4.9               3.0  ...               0.2     0.0\n",
            "2                  4.7               3.2  ...               0.2     0.0\n",
            "3                  4.6               3.1  ...               0.2     0.0\n",
            "4                  5.0               3.6  ...               0.2     0.0\n",
            "..                 ...               ...  ...               ...     ...\n",
            "145                6.7               3.0  ...               2.3     2.0\n",
            "146                6.3               2.5  ...               1.9     2.0\n",
            "147                6.5               3.0  ...               2.0     2.0\n",
            "148                6.2               3.4  ...               2.3     2.0\n",
            "149                5.9               3.0  ...               1.8     2.0\n",
            "\n",
            "[150 rows x 5 columns]\n",
            "       0    1    2    3\n",
            "0    5.1  3.5  1.4  0.2\n",
            "1    4.9  3.0  1.4  0.2\n",
            "2    4.7  3.2  1.3  0.2\n",
            "3    4.6  3.1  1.5  0.2\n",
            "4    5.0  3.6  1.4  0.2\n",
            "..   ...  ...  ...  ...\n",
            "145  6.7  3.0  5.2  2.3\n",
            "146  6.3  2.5  5.0  1.9\n",
            "147  6.5  3.0  5.2  2.0\n",
            "148  6.2  3.4  5.4  2.3\n",
            "149  5.9  3.0  5.1  1.8\n",
            "\n",
            "[150 rows x 4 columns]      0\n",
            "0    0\n",
            "1    0\n",
            "2    0\n",
            "3    0\n",
            "4    0\n",
            "..  ..\n",
            "145  2\n",
            "146  2\n",
            "147  2\n",
            "148  2\n",
            "149  2\n",
            "\n",
            "[150 rows x 1 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mHRKovWObmHI",
        "outputId": "dfffa7ab-6741-4c42-f531-af393d37ae8f"
      },
      "source": [
        "#normalize the dataset\n",
        "for column in X.columns:\n",
        "    X[column] = (X[column] - X[column].min()) / (X[column].max() - X[column].min()) \n",
        "\n",
        "print(X)\n",
        "#One-vs-All\n",
        "df=X.copy()\n",
        "df['target']=Y\n",
        "d1=df.copy()\n",
        "print(d1)\n",
        "#Taking Iris-Setosa as all 1 and the rest of them as 0\n",
        "d1[\"target\"].replace({0.0: 1.0,1.0:0.0,2.0:0.0}, inplace=True)\n",
        "\n",
        "#splitting the dataset\n",
        "train1 = d1.sample(frac=0.6)\n",
        "remaining_data = d1.drop(train1.index)\n",
        "\n",
        "validation1 = remaining_data.sample(frac=0.5)\n",
        "test1 = remaining_data.drop(validation1.index)\n",
        "\n",
        "\n",
        "print(\"\\nTRAIN SET\")\n",
        "print(train1)\n",
        "print(\"\\nVALIDATION SET\")\n",
        "print(validation1)\n",
        "print(\"\\nTEST SET\")\n",
        "print(test1)\n",
        "\n",
        "d2=df.copy()\n",
        "print(d2)\n",
        "#Taking Iris-Versicolour as all 1 and the rest of them as 0\n",
        "d2[\"target\"].replace({0.0: 0.0,1.0:1.0,2.0:0.0}, inplace=True)\n",
        "\n",
        "#splitting the dataset\n",
        "train2 = d2.sample(frac=0.6)\n",
        "remaining_data = d2.drop(train2.index)\n",
        "\n",
        "validation2 = remaining_data.sample(frac=0.5)\n",
        "test2 = remaining_data.drop(validation2.index)\n",
        "\n",
        "\n",
        "print(\"\\nTRAIN SET\")\n",
        "print(train2)\n",
        "print(\"\\nVALIDATION SET\")\n",
        "print(validation2)\n",
        "print(\"\\nTEST SET\")\n",
        "print(test2)\n",
        "\n",
        "d3=df.copy()\n",
        "print(d3)\n",
        "#Taking Iris-Virginica as all 1 and the rest of them as 0\n",
        "d3[\"target\"].replace({0.0: 0.0,1.0:0.0,2.0:1.0}, inplace=True)\n",
        "\n",
        "#splitting the dataset\n",
        "train3 = d3.sample(frac=0.6)\n",
        "remaining_data = d3.drop(train3.index)\n",
        "\n",
        "validation3 = remaining_data.sample(frac=0.5)\n",
        "test3 = remaining_data.drop(validation3.index)\n",
        "\n",
        "\n",
        "print(\"\\nTRAIN SET\")\n",
        "print(train3)\n",
        "print(\"\\nVALIDATION SET\")\n",
        "print(validation3)\n",
        "print(\"\\nTEST SET\")\n",
        "print(test3)\n",
        "print(df)\n"
      ],
      "execution_count": 244,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            0         1         2         3\n",
            "0    0.222222  0.625000  0.067797  0.041667\n",
            "1    0.166667  0.416667  0.067797  0.041667\n",
            "2    0.111111  0.500000  0.050847  0.041667\n",
            "3    0.083333  0.458333  0.084746  0.041667\n",
            "4    0.194444  0.666667  0.067797  0.041667\n",
            "..        ...       ...       ...       ...\n",
            "145  0.666667  0.416667  0.711864  0.916667\n",
            "146  0.555556  0.208333  0.677966  0.750000\n",
            "147  0.611111  0.416667  0.711864  0.791667\n",
            "148  0.527778  0.583333  0.745763  0.916667\n",
            "149  0.444444  0.416667  0.694915  0.708333\n",
            "\n",
            "[150 rows x 4 columns]\n",
            "            0         1         2         3  target\n",
            "0    0.222222  0.625000  0.067797  0.041667       0\n",
            "1    0.166667  0.416667  0.067797  0.041667       0\n",
            "2    0.111111  0.500000  0.050847  0.041667       0\n",
            "3    0.083333  0.458333  0.084746  0.041667       0\n",
            "4    0.194444  0.666667  0.067797  0.041667       0\n",
            "..        ...       ...       ...       ...     ...\n",
            "145  0.666667  0.416667  0.711864  0.916667       2\n",
            "146  0.555556  0.208333  0.677966  0.750000       2\n",
            "147  0.611111  0.416667  0.711864  0.791667       2\n",
            "148  0.527778  0.583333  0.745763  0.916667       2\n",
            "149  0.444444  0.416667  0.694915  0.708333       2\n",
            "\n",
            "[150 rows x 5 columns]\n",
            "\n",
            "TRAIN SET\n",
            "            0         1         2         3  target\n",
            "76   0.694444  0.333333  0.644068  0.541667     0.0\n",
            "52   0.722222  0.458333  0.661017  0.583333     0.0\n",
            "127  0.500000  0.416667  0.661017  0.708333     0.0\n",
            "60   0.194444  0.000000  0.423729  0.375000     0.0\n",
            "12   0.138889  0.416667  0.067797  0.000000     1.0\n",
            "..        ...       ...       ...       ...     ...\n",
            "53   0.333333  0.125000  0.508475  0.500000     0.0\n",
            "129  0.805556  0.416667  0.813559  0.625000     0.0\n",
            "100  0.555556  0.541667  0.847458  1.000000     0.0\n",
            "108  0.666667  0.208333  0.813559  0.708333     0.0\n",
            "109  0.805556  0.666667  0.864407  1.000000     0.0\n",
            "\n",
            "[90 rows x 5 columns]\n",
            "\n",
            "VALIDATION SET\n",
            "            0         1         2         3  target\n",
            "112  0.694444  0.416667  0.762712  0.833333     0.0\n",
            "18   0.388889  0.750000  0.118644  0.083333     1.0\n",
            "70   0.444444  0.500000  0.644068  0.708333     0.0\n",
            "16   0.305556  0.791667  0.050847  0.125000     1.0\n",
            "29   0.111111  0.500000  0.101695  0.041667     1.0\n",
            "148  0.527778  0.583333  0.745763  0.916667     0.0\n",
            "73   0.500000  0.333333  0.627119  0.458333     0.0\n",
            "80   0.333333  0.166667  0.474576  0.416667     0.0\n",
            "131  1.000000  0.750000  0.915254  0.791667     0.0\n",
            "65   0.666667  0.458333  0.576271  0.541667     0.0\n",
            "82   0.416667  0.291667  0.491525  0.458333     0.0\n",
            "119  0.472222  0.083333  0.677966  0.583333     0.0\n",
            "95   0.388889  0.416667  0.542373  0.458333     0.0\n",
            "40   0.194444  0.625000  0.050847  0.083333     1.0\n",
            "32   0.250000  0.875000  0.084746  0.000000     1.0\n",
            "47   0.083333  0.500000  0.067797  0.041667     1.0\n",
            "43   0.194444  0.625000  0.101695  0.208333     1.0\n",
            "130  0.861111  0.333333  0.864407  0.750000     0.0\n",
            "117  0.944444  0.750000  0.966102  0.875000     0.0\n",
            "50   0.750000  0.500000  0.627119  0.541667     0.0\n",
            "24   0.138889  0.583333  0.152542  0.041667     1.0\n",
            "94   0.361111  0.291667  0.542373  0.500000     0.0\n",
            "85   0.472222  0.583333  0.593220  0.625000     0.0\n",
            "124  0.666667  0.541667  0.796610  0.833333     0.0\n",
            "115  0.583333  0.500000  0.728814  0.916667     0.0\n",
            "11   0.138889  0.583333  0.101695  0.041667     1.0\n",
            "46   0.222222  0.750000  0.101695  0.041667     1.0\n",
            "31   0.305556  0.583333  0.084746  0.125000     1.0\n",
            "69   0.361111  0.208333  0.491525  0.416667     0.0\n",
            "44   0.222222  0.750000  0.152542  0.125000     1.0\n",
            "\n",
            "TEST SET\n",
            "            0         1         2         3  target\n",
            "1    0.166667  0.416667  0.067797  0.041667     1.0\n",
            "8    0.027778  0.375000  0.067797  0.041667     1.0\n",
            "26   0.194444  0.583333  0.101695  0.125000     1.0\n",
            "33   0.333333  0.916667  0.067797  0.041667     1.0\n",
            "36   0.333333  0.625000  0.050847  0.041667     1.0\n",
            "49   0.194444  0.541667  0.067797  0.041667     1.0\n",
            "51   0.583333  0.500000  0.593220  0.583333     0.0\n",
            "56   0.555556  0.541667  0.627119  0.625000     0.0\n",
            "61   0.444444  0.416667  0.542373  0.583333     0.0\n",
            "66   0.361111  0.416667  0.593220  0.583333     0.0\n",
            "67   0.416667  0.291667  0.525424  0.375000     0.0\n",
            "72   0.555556  0.208333  0.661017  0.583333     0.0\n",
            "83   0.472222  0.291667  0.694915  0.625000     0.0\n",
            "89   0.333333  0.208333  0.508475  0.500000     0.0\n",
            "91   0.500000  0.416667  0.610169  0.541667     0.0\n",
            "102  0.777778  0.416667  0.830508  0.833333     0.0\n",
            "103  0.555556  0.375000  0.779661  0.708333     0.0\n",
            "111  0.583333  0.291667  0.728814  0.750000     0.0\n",
            "114  0.416667  0.333333  0.694915  0.958333     0.0\n",
            "116  0.611111  0.416667  0.762712  0.708333     0.0\n",
            "118  0.944444  0.250000  1.000000  0.916667     0.0\n",
            "121  0.361111  0.333333  0.661017  0.791667     0.0\n",
            "128  0.583333  0.333333  0.779661  0.833333     0.0\n",
            "133  0.555556  0.333333  0.694915  0.583333     0.0\n",
            "135  0.944444  0.416667  0.864407  0.916667     0.0\n",
            "138  0.472222  0.416667  0.644068  0.708333     0.0\n",
            "139  0.722222  0.458333  0.745763  0.833333     0.0\n",
            "140  0.666667  0.458333  0.779661  0.958333     0.0\n",
            "143  0.694444  0.500000  0.830508  0.916667     0.0\n",
            "149  0.444444  0.416667  0.694915  0.708333     0.0\n",
            "            0         1         2         3  target\n",
            "0    0.222222  0.625000  0.067797  0.041667       0\n",
            "1    0.166667  0.416667  0.067797  0.041667       0\n",
            "2    0.111111  0.500000  0.050847  0.041667       0\n",
            "3    0.083333  0.458333  0.084746  0.041667       0\n",
            "4    0.194444  0.666667  0.067797  0.041667       0\n",
            "..        ...       ...       ...       ...     ...\n",
            "145  0.666667  0.416667  0.711864  0.916667       2\n",
            "146  0.555556  0.208333  0.677966  0.750000       2\n",
            "147  0.611111  0.416667  0.711864  0.791667       2\n",
            "148  0.527778  0.583333  0.745763  0.916667       2\n",
            "149  0.444444  0.416667  0.694915  0.708333       2\n",
            "\n",
            "[150 rows x 5 columns]\n",
            "\n",
            "TRAIN SET\n",
            "            0         1         2         3  target\n",
            "34   0.166667  0.458333  0.084746  0.041667     0.0\n",
            "130  0.861111  0.333333  0.864407  0.750000     0.0\n",
            "30   0.138889  0.458333  0.101695  0.041667     0.0\n",
            "23   0.222222  0.541667  0.118644  0.166667     0.0\n",
            "29   0.111111  0.500000  0.101695  0.041667     0.0\n",
            "..        ...       ...       ...       ...     ...\n",
            "84   0.305556  0.416667  0.593220  0.583333     1.0\n",
            "38   0.027778  0.416667  0.050847  0.041667     0.0\n",
            "4    0.194444  0.666667  0.067797  0.041667     0.0\n",
            "145  0.666667  0.416667  0.711864  0.916667     0.0\n",
            "127  0.500000  0.416667  0.661017  0.708333     0.0\n",
            "\n",
            "[90 rows x 5 columns]\n",
            "\n",
            "VALIDATION SET\n",
            "            0         1         2         3  target\n",
            "71   0.500000  0.333333  0.508475  0.500000     1.0\n",
            "99   0.388889  0.333333  0.525424  0.500000     1.0\n",
            "7    0.194444  0.583333  0.084746  0.041667     0.0\n",
            "47   0.083333  0.500000  0.067797  0.041667     0.0\n",
            "16   0.305556  0.791667  0.050847  0.125000     0.0\n",
            "95   0.388889  0.416667  0.542373  0.458333     1.0\n",
            "20   0.305556  0.583333  0.118644  0.041667     0.0\n",
            "27   0.250000  0.625000  0.084746  0.041667     0.0\n",
            "15   0.388889  1.000000  0.084746  0.125000     0.0\n",
            "25   0.194444  0.416667  0.101695  0.041667     0.0\n",
            "101  0.416667  0.291667  0.694915  0.750000     0.0\n",
            "106  0.166667  0.208333  0.593220  0.666667     0.0\n",
            "135  0.944444  0.416667  0.864407  0.916667     0.0\n",
            "22   0.083333  0.666667  0.000000  0.041667     0.0\n",
            "73   0.500000  0.333333  0.627119  0.458333     1.0\n",
            "117  0.944444  0.750000  0.966102  0.875000     0.0\n",
            "126  0.527778  0.333333  0.644068  0.708333     0.0\n",
            "40   0.194444  0.625000  0.050847  0.083333     0.0\n",
            "46   0.222222  0.750000  0.101695  0.041667     0.0\n",
            "48   0.277778  0.708333  0.084746  0.041667     0.0\n",
            "132  0.583333  0.333333  0.779661  0.875000     0.0\n",
            "103  0.555556  0.375000  0.779661  0.708333     0.0\n",
            "112  0.694444  0.416667  0.762712  0.833333     0.0\n",
            "32   0.250000  0.875000  0.084746  0.000000     0.0\n",
            "100  0.555556  0.541667  0.847458  1.000000     0.0\n",
            "69   0.361111  0.208333  0.491525  0.416667     1.0\n",
            "14   0.416667  0.833333  0.033898  0.041667     0.0\n",
            "133  0.555556  0.333333  0.694915  0.583333     0.0\n",
            "108  0.666667  0.208333  0.813559  0.708333     0.0\n",
            "81   0.333333  0.166667  0.457627  0.375000     1.0\n",
            "\n",
            "TEST SET\n",
            "            0         1         2         3  target\n",
            "3    0.083333  0.458333  0.084746  0.041667     0.0\n",
            "9    0.166667  0.458333  0.084746  0.000000     0.0\n",
            "11   0.138889  0.583333  0.101695  0.041667     0.0\n",
            "17   0.222222  0.625000  0.067797  0.083333     0.0\n",
            "39   0.222222  0.583333  0.084746  0.041667     0.0\n",
            "42   0.027778  0.500000  0.050847  0.041667     0.0\n",
            "43   0.194444  0.625000  0.101695  0.208333     0.0\n",
            "56   0.555556  0.541667  0.627119  0.625000     1.0\n",
            "59   0.250000  0.291667  0.491525  0.541667     1.0\n",
            "62   0.472222  0.083333  0.508475  0.375000     1.0\n",
            "67   0.416667  0.291667  0.525424  0.375000     1.0\n",
            "68   0.527778  0.083333  0.593220  0.583333     1.0\n",
            "79   0.388889  0.250000  0.423729  0.375000     1.0\n",
            "82   0.416667  0.291667  0.491525  0.458333     1.0\n",
            "83   0.472222  0.291667  0.694915  0.625000     1.0\n",
            "87   0.555556  0.125000  0.576271  0.500000     1.0\n",
            "88   0.361111  0.416667  0.525424  0.500000     1.0\n",
            "91   0.500000  0.416667  0.610169  0.541667     1.0\n",
            "96   0.388889  0.375000  0.542373  0.500000     1.0\n",
            "97   0.527778  0.375000  0.559322  0.500000     1.0\n",
            "102  0.777778  0.416667  0.830508  0.833333     0.0\n",
            "111  0.583333  0.291667  0.728814  0.750000     0.0\n",
            "115  0.583333  0.500000  0.728814  0.916667     0.0\n",
            "118  0.944444  0.250000  1.000000  0.916667     0.0\n",
            "124  0.666667  0.541667  0.796610  0.833333     0.0\n",
            "131  1.000000  0.750000  0.915254  0.791667     0.0\n",
            "134  0.500000  0.250000  0.779661  0.541667     0.0\n",
            "146  0.555556  0.208333  0.677966  0.750000     0.0\n",
            "147  0.611111  0.416667  0.711864  0.791667     0.0\n",
            "148  0.527778  0.583333  0.745763  0.916667     0.0\n",
            "            0         1         2         3  target\n",
            "0    0.222222  0.625000  0.067797  0.041667       0\n",
            "1    0.166667  0.416667  0.067797  0.041667       0\n",
            "2    0.111111  0.500000  0.050847  0.041667       0\n",
            "3    0.083333  0.458333  0.084746  0.041667       0\n",
            "4    0.194444  0.666667  0.067797  0.041667       0\n",
            "..        ...       ...       ...       ...     ...\n",
            "145  0.666667  0.416667  0.711864  0.916667       2\n",
            "146  0.555556  0.208333  0.677966  0.750000       2\n",
            "147  0.611111  0.416667  0.711864  0.791667       2\n",
            "148  0.527778  0.583333  0.745763  0.916667       2\n",
            "149  0.444444  0.416667  0.694915  0.708333       2\n",
            "\n",
            "[150 rows x 5 columns]\n",
            "\n",
            "TRAIN SET\n",
            "            0         1         2         3  target\n",
            "112  0.694444  0.416667  0.762712  0.833333     1.0\n",
            "131  1.000000  0.750000  0.915254  0.791667     1.0\n",
            "125  0.805556  0.500000  0.847458  0.708333     1.0\n",
            "113  0.388889  0.208333  0.677966  0.791667     1.0\n",
            "14   0.416667  0.833333  0.033898  0.041667     0.0\n",
            "..        ...       ...       ...       ...     ...\n",
            "126  0.527778  0.333333  0.644068  0.708333     1.0\n",
            "6    0.083333  0.583333  0.067797  0.083333     0.0\n",
            "95   0.388889  0.416667  0.542373  0.458333     0.0\n",
            "135  0.944444  0.416667  0.864407  0.916667     1.0\n",
            "138  0.472222  0.416667  0.644068  0.708333     1.0\n",
            "\n",
            "[90 rows x 5 columns]\n",
            "\n",
            "VALIDATION SET\n",
            "            0         1         2         3  target\n",
            "79   0.388889  0.250000  0.423729  0.375000     0.0\n",
            "86   0.666667  0.458333  0.627119  0.583333     0.0\n",
            "55   0.388889  0.333333  0.593220  0.500000     0.0\n",
            "7    0.194444  0.583333  0.084746  0.041667     0.0\n",
            "0    0.222222  0.625000  0.067797  0.041667     0.0\n",
            "43   0.194444  0.625000  0.101695  0.208333     0.0\n",
            "127  0.500000  0.416667  0.661017  0.708333     1.0\n",
            "102  0.777778  0.416667  0.830508  0.833333     1.0\n",
            "33   0.333333  0.916667  0.067797  0.041667     0.0\n",
            "114  0.416667  0.333333  0.694915  0.958333     1.0\n",
            "84   0.305556  0.416667  0.593220  0.583333     0.0\n",
            "51   0.583333  0.500000  0.593220  0.583333     0.0\n",
            "24   0.138889  0.583333  0.152542  0.041667     0.0\n",
            "38   0.027778  0.416667  0.050847  0.041667     0.0\n",
            "10   0.305556  0.708333  0.084746  0.041667     0.0\n",
            "39   0.222222  0.583333  0.084746  0.041667     0.0\n",
            "46   0.222222  0.750000  0.101695  0.041667     0.0\n",
            "37   0.166667  0.666667  0.067797  0.000000     0.0\n",
            "47   0.083333  0.500000  0.067797  0.041667     0.0\n",
            "92   0.416667  0.250000  0.508475  0.458333     0.0\n",
            "87   0.555556  0.125000  0.576271  0.500000     0.0\n",
            "139  0.722222  0.458333  0.745763  0.833333     1.0\n",
            "60   0.194444  0.000000  0.423729  0.375000     0.0\n",
            "110  0.611111  0.500000  0.694915  0.791667     1.0\n",
            "78   0.472222  0.375000  0.593220  0.583333     0.0\n",
            "72   0.555556  0.208333  0.661017  0.583333     0.0\n",
            "19   0.222222  0.750000  0.084746  0.083333     0.0\n",
            "89   0.333333  0.208333  0.508475  0.500000     0.0\n",
            "134  0.500000  0.250000  0.779661  0.541667     1.0\n",
            "83   0.472222  0.291667  0.694915  0.625000     0.0\n",
            "\n",
            "TEST SET\n",
            "            0         1         2         3  target\n",
            "5    0.305556  0.791667  0.118644  0.125000     0.0\n",
            "9    0.166667  0.458333  0.084746  0.000000     0.0\n",
            "18   0.388889  0.750000  0.118644  0.083333     0.0\n",
            "25   0.194444  0.416667  0.101695  0.041667     0.0\n",
            "30   0.138889  0.458333  0.101695  0.041667     0.0\n",
            "44   0.222222  0.750000  0.152542  0.125000     0.0\n",
            "58   0.638889  0.375000  0.610169  0.500000     0.0\n",
            "59   0.250000  0.291667  0.491525  0.541667     0.0\n",
            "62   0.472222  0.083333  0.508475  0.375000     0.0\n",
            "66   0.361111  0.416667  0.593220  0.583333     0.0\n",
            "75   0.638889  0.416667  0.576271  0.541667     0.0\n",
            "81   0.333333  0.166667  0.457627  0.375000     0.0\n",
            "88   0.361111  0.416667  0.525424  0.500000     0.0\n",
            "93   0.194444  0.125000  0.389831  0.375000     0.0\n",
            "99   0.388889  0.333333  0.525424  0.500000     0.0\n",
            "100  0.555556  0.541667  0.847458  1.000000     1.0\n",
            "103  0.555556  0.375000  0.779661  0.708333     1.0\n",
            "105  0.916667  0.416667  0.949153  0.833333     1.0\n",
            "106  0.166667  0.208333  0.593220  0.666667     1.0\n",
            "109  0.805556  0.666667  0.864407  1.000000     1.0\n",
            "118  0.944444  0.250000  1.000000  0.916667     1.0\n",
            "120  0.722222  0.500000  0.796610  0.916667     1.0\n",
            "122  0.944444  0.333333  0.966102  0.791667     1.0\n",
            "124  0.666667  0.541667  0.796610  0.833333     1.0\n",
            "128  0.583333  0.333333  0.779661  0.833333     1.0\n",
            "130  0.861111  0.333333  0.864407  0.750000     1.0\n",
            "140  0.666667  0.458333  0.779661  0.958333     1.0\n",
            "142  0.416667  0.291667  0.694915  0.750000     1.0\n",
            "145  0.666667  0.416667  0.711864  0.916667     1.0\n",
            "146  0.555556  0.208333  0.677966  0.750000     1.0\n",
            "            0         1         2         3  target\n",
            "0    0.222222  0.625000  0.067797  0.041667       0\n",
            "1    0.166667  0.416667  0.067797  0.041667       0\n",
            "2    0.111111  0.500000  0.050847  0.041667       0\n",
            "3    0.083333  0.458333  0.084746  0.041667       0\n",
            "4    0.194444  0.666667  0.067797  0.041667       0\n",
            "..        ...       ...       ...       ...     ...\n",
            "145  0.666667  0.416667  0.711864  0.916667       2\n",
            "146  0.555556  0.208333  0.677966  0.750000       2\n",
            "147  0.611111  0.416667  0.711864  0.791667       2\n",
            "148  0.527778  0.583333  0.745763  0.916667       2\n",
            "149  0.444444  0.416667  0.694915  0.708333       2\n",
            "\n",
            "[150 rows x 5 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADf3X1OEe-z1"
      },
      "source": [
        "def logistic_regression(dataset,learning_rate,rho,epoch):\n",
        "  \n",
        "  rows = len(dataset.axes[0])\n",
        "  cols = len(dataset.axes[1])\n",
        "  X = dataset.iloc[:, :-1]\n",
        "  Y = dataset.iloc[:, -1]\n",
        "\n",
        "  X.insert(0, \"DEFAULT\",1, True)\n",
        "  X_arr=X.to_numpy()\n",
        "  Y_arr=Y.to_numpy()\n",
        "  \n",
        " # X_arr=np.append(X_arr,0, axis=0)\n",
        "  w=[]\n",
        "  for i in range(cols):\n",
        "    w.append(random.uniform(-0.3,0.3))\n",
        "\n",
        "  w= np.array(w)  \n",
        "  J_w=0\n",
        "  J_w_in=0\n",
        "  diff_J=0\n",
        "  l=learning_rate\n",
        "  m=len(X_arr)\n",
        "  for i in range(epoch):\n",
        "    \n",
        "    h_x=1/(1+np.exp(-(np.dot(X_arr,w))))\n",
        "    for k in range(m):\n",
        "        J_w_in= J_w_in+Y_arr[k]*math.log(h_x[k])+(1-Y_arr[k])*math.log(1-h_x[k])\n",
        "    \n",
        "    J_w_in=-J_w_in/m\n",
        "\n",
        "    for j in range(len(w)):\n",
        "        for k in range(m):\n",
        "       \n",
        "            diff_J=(h_x[k]-Y_arr[k])*X_arr[k][j]\n",
        "        w[j]=w[j]-(l*diff_J)/m\n",
        "\n",
        "    \n",
        "    h_x=1/(1+np.exp(-(np.dot(X_arr,w))))   \n",
        "    for k in range(m):\n",
        "        J_w= J_w+Y_arr[k]*math.log(h_x[k])+(1-Y_arr[k])*math.log(1-h_x[k])\n",
        "\n",
        "    J_w=-J_w/m\n",
        "    #if(abs(J_w-J_w_in)>rho):\n",
        "      #break\n",
        "  return w\n",
        "\n"
      ],
      "execution_count": 245,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "visRGq6gmH9e",
        "outputId": "faa80bf6-30ee-4a57-f226-b08477bee2f3"
      },
      "source": [
        "#Training all the models\n",
        "h1=logistic_regression(train1,0.2,0.001,10)\n",
        "print(h1)\n",
        "\n",
        "h2=logistic_regression(train2,0.2,0.001,10)\n",
        "print(h2)\n",
        "\n",
        "h3=logistic_regression(train3,0.2,0.001,10)\n",
        "print(h3)"
      ],
      "execution_count": 246,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0.1675904   0.06855987  0.17056989  0.2269862  -0.23977642]\n",
            "[-0.01934813  0.28936934 -0.2228781  -0.04654576 -0.15722208]\n",
            "[-0.121424    0.01526674  0.12085352 -0.03395192  0.10935956]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_FCMGqBjqRTz",
        "outputId": "29f8dcf3-c80f-4942-8f2c-458027df43c2"
      },
      "source": [
        "h=[h1,h2,h3]\n",
        "\n",
        "def predict(dataset,weights):\n",
        "  X = dataset.iloc[:, :-1]\n",
        "  Y = dataset.iloc[:, -1]\n",
        "\n",
        "  X.insert(0, \"DEFAULT\",1, True)\n",
        "  X_arr=X.to_numpy()\n",
        "  Y_arr=Y.to_numpy()\n",
        "  Y_predict=np.zeros(len(Y_arr))\n",
        "  \n",
        "  Z1= 1/(1+ np.exp(-(np.dot(X_arr,weights[0]))))\n",
        "  Z2= 1/(1+ np.exp(-(np.dot(X_arr,weights[1]))))\n",
        "  Z3= 1/(1+ np.exp(-(np.dot(X_arr,weights[2]))))\n",
        "  z=[Z1,Z2,Z3]\n",
        "  for i in range(len(Z1)):\n",
        "     m_a_x=float('-inf')\n",
        "     pred=0.0\n",
        "     for j in range(len(z)):\n",
        "        if(m_a_x<z[j][i]):\n",
        "          m_a_x=z[j][i]\n",
        "          pred=j\n",
        "        \n",
        "     Y_predict[i]=pred\n",
        "  count=0\n",
        "  for i in range(np.size(Y_predict)):\n",
        "    if (Y_predict[i]==Y_arr[i]):\n",
        "      count=count+1;\n",
        "    \n",
        "  return Y_predict,(count/np.size(Y_predict))*100\n",
        "\n",
        "\n",
        "#splitting the dataset\n",
        "df=X.copy()\n",
        "df['target']=Y\n",
        "print(df)\n",
        "train = df.sample(frac=0.6)\n",
        "remaining_data = df.drop(train.index)\n",
        "\n",
        "validation = remaining_data.sample(frac=0.5)\n",
        "test = remaining_data.drop(validation.index)\n",
        "\n",
        "print(predict(train,h)[1])\n"
      ],
      "execution_count": 268,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            0         1         2         3  target\n",
            "0    0.222222  0.625000  0.067797  0.041667       0\n",
            "1    0.166667  0.416667  0.067797  0.041667       0\n",
            "2    0.111111  0.500000  0.050847  0.041667       0\n",
            "3    0.083333  0.458333  0.084746  0.041667       0\n",
            "4    0.194444  0.666667  0.067797  0.041667       0\n",
            "..        ...       ...       ...       ...     ...\n",
            "145  0.666667  0.416667  0.711864  0.916667       2\n",
            "146  0.555556  0.208333  0.677966  0.750000       2\n",
            "147  0.611111  0.416667  0.711864  0.791667       2\n",
            "148  0.527778  0.583333  0.745763  0.916667       2\n",
            "149  0.444444  0.416667  0.694915  0.708333       2\n",
            "\n",
            "[150 rows x 5 columns]\n",
            "33.33333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iZH4vq6rsmkT",
        "outputId": "4c15c8ba-bab3-415f-c869-869f2e3aad53"
      },
      "source": [
        "#Hyperparameter Tuning\n",
        "h1=logistic_regression(validation1,0.2,0.001,10)\n",
        "\n",
        "h2=logistic_regression(validation2,0.2,0.001,10)\n",
        "\n",
        "h3=logistic_regression(validation3,0.2,0.001,10)\n",
        "h=[h1,h2,h3]\n",
        "print(\"Accuracy: \")\n",
        "print(predict(validation,h)[1])"
      ],
      "execution_count": 267,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: \n",
            "33.33333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ISp5g3Lqzv5T",
        "outputId": "7096914e-f0a7-46de-ab79-89421d19c58d"
      },
      "source": [
        "h1=logistic_regression(validation1,0.2,0.001,15)\n",
        "\n",
        "h2=logistic_regression(validation2,0.2,0.001,15)\n",
        "\n",
        "h3=logistic_regression(validation3,0.2,0.001,15)\n",
        "h=[h1,h2,h3]\n",
        "print(\"Accuracy: \")\n",
        "print(predict(validation,h)[1])"
      ],
      "execution_count": 266,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: \n",
            "40.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djNcTO0J729_",
        "outputId": "12e6afbf-5fb1-418f-cdf9-af32fd4b91ee"
      },
      "source": [
        "h1=logistic_regression(validation1,0.2,0.001,50)\n",
        "\n",
        "h2=logistic_regression(validation2,0.2,0.001,50)\n",
        "\n",
        "h3=logistic_regression(validation3,0.2,0.001,50)\n",
        "h=[h1,h2,h3]\n",
        "print(\"Accuracy: \")\n",
        "print(predict(validation,h)[1])"
      ],
      "execution_count": 265,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: \n",
            "30.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLONnDC29VkO",
        "outputId": "8b7896f4-32e8-4fbd-cf8b-fe805531aaaf"
      },
      "source": [
        "h1=logistic_regression(validation1,0.2,0.001,7)\n",
        "\n",
        "h2=logistic_regression(validation2,0.2,0.001,7)\n",
        "\n",
        "h3=logistic_regression(validation3,0.2,0.001,7)\n",
        "h=[h1,h2,h3]\n",
        "print(\"Accuracy: \")\n",
        "print(predict(validation,h)[1])"
      ],
      "execution_count": 264,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: \n",
            "20.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Re41ErbM9YJ_",
        "outputId": "a3bc694b-1ac8-4113-aae8-9c34ea58a2be"
      },
      "source": [
        "h1=logistic_regression(validation1,0.2,0.001,3)\n",
        "\n",
        "h2=logistic_regression(validation2,0.2,0.001,3)\n",
        "\n",
        "h3=logistic_regression(validation3,0.2,0.001,3)\n",
        "h=[h1,h2,h3]\n",
        "print(\"Accuracy: \")\n",
        "print(predict(validation,h)[1])"
      ],
      "execution_count": 263,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: \n",
            "36.666666666666664\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "TD80_oyX9jHp",
        "outputId": "b5c0e34e-a8a4-471b-c535-02e242e47715"
      },
      "source": [
        "#checking for overfitting\n",
        "epochs=[10,20,30,40,50,60,70,80,90]\n",
        "train_acc=[]\n",
        "valid_acc=[]\n",
        "\n",
        "for i in epochs:\n",
        "    h1=logistic_regression(train1,0.2,0.001,i)\n",
        "\n",
        "    h2=logistic_regression(train2,0.2,0.001,i)\n",
        "\n",
        "    h3=logistic_regression(train3,0.2,0.001,i)\n",
        "    h=[h1,h2,h3]\n",
        "    train_acc.append(predict(train,h)[1])\n",
        "    valid_acc.append(predict(validation,h)[1])\n",
        "\n",
        "plt.plot(epochs,train_acc, label = \"Training Accuracy\")\n",
        "plt.plot(epochs,valid_acc, label = \"Validation Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()  "
      ],
      "execution_count": 259,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 951
        },
        "id": "yHn7UpZS5Xja",
        "outputId": "36d9f293-4e6e-4a66-d380-4b719878e8a9"
      },
      "source": [
        "#classification accuracy for different percentage of training samples\n",
        "for i in range(10):\n",
        "  train = df.sample(frac=((i+1)/10))\n",
        "  test = df.drop(train.index)\n",
        "  h1=logistic_regression(train,0.2,0.001,40)\n",
        "  h2=logistic_regression(train,0.2,0.001,40)\n",
        "  h3=logistic_regression(train,0.2,0.001,40)\n",
        "  h=[h1,h2,h3]\n",
        "  pred1=predict(train,h)\n",
        "  pred2=predict(test,h)\n",
        "  print(\"Train Accuracy for \",(i+1)*10,\"percentage of training samples: \",pred1[1])\n",
        "  print(\"Test Accuracy for \",(i+1)*10,\"percentage of training samples: \",pred2[1])\n",
        "  print(\"\\n\")"
      ],
      "execution_count": 262,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy for  10 percentage of training samples:  33.33333333333333\n",
            "Test Accuracy for  10 percentage of training samples:  33.33333333333333\n",
            "\n",
            "\n",
            "Train Accuracy for  20 percentage of training samples:  30.0\n",
            "Test Accuracy for  20 percentage of training samples:  34.166666666666664\n",
            "\n",
            "\n",
            "Train Accuracy for  30 percentage of training samples:  26.666666666666668\n",
            "Test Accuracy for  30 percentage of training samples:  36.19047619047619\n",
            "\n",
            "\n",
            "Train Accuracy for  40 percentage of training samples:  36.666666666666664\n",
            "Test Accuracy for  40 percentage of training samples:  31.11111111111111\n",
            "\n",
            "\n",
            "Train Accuracy for  50 percentage of training samples:  36.0\n",
            "Test Accuracy for  50 percentage of training samples:  28.000000000000004\n",
            "\n",
            "\n",
            "Train Accuracy for  60 percentage of training samples:  2.2222222222222223\n",
            "Test Accuracy for  60 percentage of training samples:  0.0\n",
            "\n",
            "\n",
            "Train Accuracy for  70 percentage of training samples:  34.285714285714285\n",
            "Test Accuracy for  70 percentage of training samples:  31.11111111111111\n",
            "\n",
            "\n",
            "Train Accuracy for  80 percentage of training samples:  34.166666666666664\n",
            "Test Accuracy for  80 percentage of training samples:  30.0\n",
            "\n",
            "\n",
            "Train Accuracy for  90 percentage of training samples:  34.81481481481482\n",
            "Test Accuracy for  90 percentage of training samples:  20.0\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ZeroDivisionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-262-c05256906716>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mh1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0mpred1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m   \u001b[0mpred2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Train Accuracy for \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"percentage of training samples: \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpred1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Test Accuracy for \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"percentage of training samples: \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpred2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-247-6584442214d6>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(dataset, weights)\u001b[0m\n\u001b[1;32m     28\u001b[0m       \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mY_predict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_predict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
          ]
        }
      ]
    }
  ]
}