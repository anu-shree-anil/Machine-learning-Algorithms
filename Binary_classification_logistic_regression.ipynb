{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Binary_classification_logistic_regression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNppF/E3MJ9R2U6GG3DxQpg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anu-shree-anil/Machine-learning-Algorithms/blob/main/Binary_classification_logistic_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTemmQUZxM-e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85e377ba-bf07-4728-fc90-af4d48ae8257"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import math\n",
        "\n",
        "#loading and preprocessing the data\n",
        "iris=datasets.load_iris()\n",
        "df=pd.DataFrame(data=np.c_[iris['data'],iris['target']],columns=iris['feature_names']+['target'])\n",
        "print(\"ORIGINAL DATA:\")\n",
        "print(df)\n",
        "df=df[df.target!=2.0]\n",
        "print(\"PREPROCESSED DATA:\")\n",
        "print(df)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ORIGINAL DATA:\n",
            "     sepal length (cm)  sepal width (cm)  ...  petal width (cm)  target\n",
            "0                  5.1               3.5  ...               0.2     0.0\n",
            "1                  4.9               3.0  ...               0.2     0.0\n",
            "2                  4.7               3.2  ...               0.2     0.0\n",
            "3                  4.6               3.1  ...               0.2     0.0\n",
            "4                  5.0               3.6  ...               0.2     0.0\n",
            "..                 ...               ...  ...               ...     ...\n",
            "145                6.7               3.0  ...               2.3     2.0\n",
            "146                6.3               2.5  ...               1.9     2.0\n",
            "147                6.5               3.0  ...               2.0     2.0\n",
            "148                6.2               3.4  ...               2.3     2.0\n",
            "149                5.9               3.0  ...               1.8     2.0\n",
            "\n",
            "[150 rows x 5 columns]\n",
            "PREPROCESSED DATA:\n",
            "    sepal length (cm)  sepal width (cm)  ...  petal width (cm)  target\n",
            "0                 5.1               3.5  ...               0.2     0.0\n",
            "1                 4.9               3.0  ...               0.2     0.0\n",
            "2                 4.7               3.2  ...               0.2     0.0\n",
            "3                 4.6               3.1  ...               0.2     0.0\n",
            "4                 5.0               3.6  ...               0.2     0.0\n",
            "..                ...               ...  ...               ...     ...\n",
            "95                5.7               3.0  ...               1.2     1.0\n",
            "96                5.7               2.9  ...               1.3     1.0\n",
            "97                6.2               2.9  ...               1.3     1.0\n",
            "98                5.1               2.5  ...               1.1     1.0\n",
            "99                5.7               2.8  ...               1.3     1.0\n",
            "\n",
            "[100 rows x 5 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yX_yIvAU3RJZ",
        "outputId": "f3db45db-a995-435b-ee62-ce9a11eeaf5b"
      },
      "source": [
        "#normalize the dataset\n",
        "for column in df.columns:\n",
        "    df[column] = (df[column] - df[column].min()) / (df[column].max() - df[column].min()) \n",
        "\n",
        "#splitting the dataset\n",
        "train = df.sample(frac=0.6)\n",
        "remaining_data = df.drop(train.index)\n",
        "\n",
        "validation = remaining_data.sample(frac=0.5)\n",
        "test = remaining_data.drop(validation.index)\n",
        "\n",
        "\n",
        "print(\"\\nTRAIN SET\")\n",
        "print(train)\n",
        "print(\"\\nVALIDATION SET\")\n",
        "print(validation)\n",
        "print(\"\\nTEST SET\")\n",
        "print(test)\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "TRAIN SET\n",
            "    sepal length (cm)  sepal width (cm)  ...  petal width (cm)  target\n",
            "20           0.407407          0.583333  ...          0.058824     0.0\n",
            "2            0.148148          0.500000  ...          0.058824     0.0\n",
            "98           0.296296          0.208333  ...          0.588235     1.0\n",
            "94           0.481481          0.291667  ...          0.705882     1.0\n",
            "0            0.296296          0.625000  ...          0.058824     0.0\n",
            "54           0.814815          0.333333  ...          0.823529     1.0\n",
            "9            0.222222          0.458333  ...          0.000000     0.0\n",
            "76           0.925926          0.333333  ...          0.764706     1.0\n",
            "69           0.481481          0.208333  ...          0.588235     1.0\n",
            "92           0.555556          0.250000  ...          0.647059     1.0\n",
            "17           0.296296          0.625000  ...          0.117647     0.0\n",
            "56           0.740741          0.541667  ...          0.882353     1.0\n",
            "71           0.666667          0.333333  ...          0.705882     1.0\n",
            "86           0.888889          0.458333  ...          0.823529     1.0\n",
            "46           0.296296          0.750000  ...          0.058824     0.0\n",
            "33           0.444444          0.916667  ...          0.058824     0.0\n",
            "42           0.037037          0.500000  ...          0.058824     0.0\n",
            "43           0.259259          0.625000  ...          0.294118     0.0\n",
            "91           0.666667          0.416667  ...          0.764706     1.0\n",
            "82           0.555556          0.291667  ...          0.647059     1.0\n",
            "51           0.777778          0.500000  ...          0.823529     1.0\n",
            "89           0.444444          0.208333  ...          0.705882     1.0\n",
            "3            0.111111          0.458333  ...          0.058824     0.0\n",
            "28           0.333333          0.583333  ...          0.058824     0.0\n",
            "32           0.333333          0.875000  ...          0.000000     0.0\n",
            "55           0.518519          0.333333  ...          0.705882     1.0\n",
            "57           0.222222          0.166667  ...          0.529412     1.0\n",
            "22           0.111111          0.666667  ...          0.058824     0.0\n",
            "30           0.185185          0.458333  ...          0.058824     0.0\n",
            "50           1.000000          0.500000  ...          0.764706     1.0\n",
            "24           0.185185          0.583333  ...          0.058824     0.0\n",
            "6            0.111111          0.583333  ...          0.117647     0.0\n",
            "93           0.259259          0.125000  ...          0.529412     1.0\n",
            "73           0.666667          0.333333  ...          0.647059     1.0\n",
            "52           0.962963          0.458333  ...          0.823529     1.0\n",
            "39           0.296296          0.583333  ...          0.058824     0.0\n",
            "75           0.851852          0.416667  ...          0.764706     1.0\n",
            "11           0.185185          0.583333  ...          0.058824     0.0\n",
            "49           0.259259          0.541667  ...          0.058824     0.0\n",
            "90           0.444444          0.250000  ...          0.647059     1.0\n",
            "53           0.444444          0.125000  ...          0.705882     1.0\n",
            "48           0.370370          0.708333  ...          0.058824     0.0\n",
            "4            0.259259          0.666667  ...          0.058824     0.0\n",
            "64           0.481481          0.375000  ...          0.705882     1.0\n",
            "59           0.333333          0.291667  ...          0.764706     1.0\n",
            "19           0.296296          0.750000  ...          0.117647     0.0\n",
            "37           0.222222          0.666667  ...          0.000000     0.0\n",
            "8            0.037037          0.375000  ...          0.058824     0.0\n",
            "1            0.222222          0.416667  ...          0.058824     0.0\n",
            "31           0.407407          0.583333  ...          0.176471     0.0\n",
            "87           0.740741          0.125000  ...          0.705882     1.0\n",
            "47           0.111111          0.500000  ...          0.058824     0.0\n",
            "85           0.629630          0.583333  ...          0.882353     1.0\n",
            "15           0.518519          1.000000  ...          0.176471     0.0\n",
            "7            0.259259          0.583333  ...          0.058824     0.0\n",
            "77           0.888889          0.416667  ...          0.941176     1.0\n",
            "96           0.518519          0.375000  ...          0.705882     1.0\n",
            "72           0.740741          0.208333  ...          0.823529     1.0\n",
            "65           0.888889          0.458333  ...          0.764706     1.0\n",
            "23           0.296296          0.541667  ...          0.235294     0.0\n",
            "\n",
            "[60 rows x 5 columns]\n",
            "\n",
            "VALIDATION SET\n",
            "    sepal length (cm)  sepal width (cm)  ...  petal width (cm)  target\n",
            "38           0.037037          0.416667  ...          0.058824     0.0\n",
            "21           0.296296          0.708333  ...          0.176471     0.0\n",
            "62           0.629630          0.083333  ...          0.529412     1.0\n",
            "81           0.444444          0.166667  ...          0.529412     1.0\n",
            "5            0.407407          0.791667  ...          0.176471     0.0\n",
            "29           0.148148          0.500000  ...          0.058824     0.0\n",
            "95           0.518519          0.416667  ...          0.647059     1.0\n",
            "25           0.259259          0.416667  ...          0.058824     0.0\n",
            "78           0.629630          0.375000  ...          0.823529     1.0\n",
            "10           0.407407          0.708333  ...          0.058824     0.0\n",
            "83           0.629630          0.291667  ...          0.882353     1.0\n",
            "70           0.592593          0.500000  ...          1.000000     1.0\n",
            "88           0.481481          0.416667  ...          0.705882     1.0\n",
            "84           0.407407          0.416667  ...          0.823529     1.0\n",
            "40           0.259259          0.625000  ...          0.117647     0.0\n",
            "99           0.518519          0.333333  ...          0.705882     1.0\n",
            "58           0.851852          0.375000  ...          0.705882     1.0\n",
            "97           0.703704          0.375000  ...          0.705882     1.0\n",
            "80           0.444444          0.166667  ...          0.588235     1.0\n",
            "68           0.703704          0.083333  ...          0.823529     1.0\n",
            "\n",
            "[20 rows x 5 columns]\n",
            "\n",
            "TEST SET\n",
            "    sepal length (cm)  sepal width (cm)  ...  petal width (cm)  target\n",
            "12           0.185185          0.416667  ...          0.000000     0.0\n",
            "13           0.000000          0.416667  ...          0.000000     0.0\n",
            "14           0.555556          0.833333  ...          0.058824     0.0\n",
            "16           0.407407          0.791667  ...          0.176471     0.0\n",
            "18           0.518519          0.750000  ...          0.117647     0.0\n",
            "26           0.259259          0.583333  ...          0.176471     0.0\n",
            "27           0.333333          0.625000  ...          0.058824     0.0\n",
            "34           0.222222          0.458333  ...          0.058824     0.0\n",
            "35           0.259259          0.500000  ...          0.058824     0.0\n",
            "36           0.444444          0.625000  ...          0.058824     0.0\n",
            "41           0.074074          0.125000  ...          0.117647     0.0\n",
            "44           0.296296          0.750000  ...          0.176471     0.0\n",
            "45           0.185185          0.416667  ...          0.117647     0.0\n",
            "60           0.259259          0.000000  ...          0.529412     1.0\n",
            "61           0.592593          0.416667  ...          0.823529     1.0\n",
            "63           0.666667          0.375000  ...          0.764706     1.0\n",
            "66           0.481481          0.416667  ...          0.823529     1.0\n",
            "67           0.555556          0.291667  ...          0.529412     1.0\n",
            "74           0.777778          0.375000  ...          0.705882     1.0\n",
            "79           0.518519          0.250000  ...          0.529412     1.0\n",
            "\n",
            "[20 rows x 5 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vu_0MzPeVQNd",
        "outputId": "7cf27485-c82f-437c-d3ab-7418684217f6"
      },
      "source": [
        "def logistic_regression(dataset,learning_rate,rho,epoch):\n",
        "  \n",
        "  rows = len(dataset.axes[0])\n",
        "  cols = len(dataset.axes[1])\n",
        "  X = dataset.iloc[:, :-1]\n",
        "  Y = dataset.iloc[:, -1]\n",
        "\n",
        "  X.insert(0, \"DEFAULT\",1, True)\n",
        "  X_arr=X.to_numpy()\n",
        "  Y_arr=Y.to_numpy()\n",
        "  \n",
        " # X_arr=np.append(X_arr,0, axis=0)\n",
        "  w=[]\n",
        "  for i in range(cols):\n",
        "    w.append(0.3)\n",
        "\n",
        "  w= np.array(w)  \n",
        "  J_w=0\n",
        "  J_w_in=0\n",
        "  diff_J=0\n",
        "  l=learning_rate\n",
        "  m=len(X_arr)\n",
        "  for i in range(epoch):\n",
        "    \n",
        "    h_x=1/(1+np.exp(-(np.dot(X_arr,w))))\n",
        "    for k in range(m):\n",
        "        J_w_in= J_w_in+Y_arr[k]*math.log(h_x[k])+(1-Y_arr[k])*math.log(1-h_x[k])\n",
        "    \n",
        "    J_w_in=-J_w_in/m\n",
        "    weight=[]\n",
        "\n",
        "    for j in range(len(w)):\n",
        "        for k in range(m):\n",
        "       \n",
        "            diff_J=(h_x[k]-Y_arr[k])*X_arr[k][j]\n",
        "        w[j]=w[j]-(l*diff_J)/m\n",
        "\n",
        "    h_x=1/(1+np.exp(-(np.dot(X_arr,w))))   \n",
        "    for k in range(m):\n",
        "        J_w= J_w+Y_arr[k]*math.log(h_x[k])+(1-Y_arr[k])*math.log(1-h_x[k])\n",
        "\n",
        "    J_w=-J_w/m\n",
        "    #if(abs(J_w-J_w_in)>rho):\n",
        "      #break\n",
        "  return w\n",
        "\n",
        "\n",
        "def predict(dataset,weights):\n",
        "  X = dataset.iloc[:, :-1]\n",
        "  Y = dataset.iloc[:, -1]\n",
        "\n",
        "  X.insert(0, \"DEFAULT\",1, True)\n",
        "  X_arr=X.to_numpy()\n",
        "  Y_arr=Y.to_numpy()\n",
        "  \n",
        "  Z= 1/(1+ np.exp(-(np.dot(X_arr,weights))))\n",
        "  Y_predict=np.where(Z>0.5,1,0)\n",
        "  count=0\n",
        "  for i in range(np.size(Y_predict)):\n",
        "    if (Y_predict[i]==Y_arr[i]):\n",
        "      count=count+1;\n",
        "    \n",
        "  return Y_predict,(count/np.size(Y_predict))*100\n",
        "\n",
        "w=logistic_regression(train,0.3,0.001,50)\n",
        "pred=predict(train,w)\n",
        "print(\"Weight:\")\n",
        "print(w)\n",
        "print(\"Accuracy: \",pred[1])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weight:\n",
            "[0.1410724  0.25291034 0.21391422 0.27286602 0.26260527]\n",
            "Accuracy:  50.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDpDpgH2Y2iq",
        "outputId": "97635b35-f3b6-49c4-e150-90b3240b514a"
      },
      "source": [
        "#Hyperparameter tuning\n",
        "w=logistic_regression(validation,0.1,0.001,50)\n",
        "pred=predict(validation,w)\n",
        "print(\"Accuracy: \",pred[1])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  65.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_9COt3F8sNV",
        "outputId": "bf057900-b8a8-43f4-9205-9fcfc6f049a5"
      },
      "source": [
        "w=logistic_regression(validation,0.5,0.001,10)\n",
        "pred=predict(validation,w)\n",
        "print(\"Accuracy: \",pred[1])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  65.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ti2s-xK8-no",
        "outputId": "5e619be2-b708-41fc-d422-9ea5e773b302"
      },
      "source": [
        "w=logistic_regression(validation,0.1,0.001,2)\n",
        "pred=predict(validation,w)\n",
        "print(\"Accuracy: \",pred[1])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  65.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WMtV4_hBbvF",
        "outputId": "c46324b9-f1ad-4057-c24e-0732c14b5b81"
      },
      "source": [
        "w=logistic_regression(validation,0.2,0.001,5)\n",
        "pred=predict(validation,w)\n",
        "print(\"Accuracy: \",pred[1])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  65.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "Sgb-LPPSBl3c",
        "outputId": "26425be0-db25-4461-fab6-830efbca500c"
      },
      "source": [
        "#checking for overfitting\n",
        "epochs=[1,2,3,4,5,6,7,8,9]\n",
        "train_acc=[]\n",
        "valid_acc=[]\n",
        "\n",
        "for i in epochs:\n",
        "    w=logistic_regression(train,0.1,0.001,i)\n",
        "    train_acc.append((predict(train,w)[1]))\n",
        "    valid_acc.append((predict(validation,w)[1]))\n",
        "\n",
        "plt.plot(epochs,train_acc, label = \"Training Accuracy\")\n",
        "plt.plot(epochs,valid_acc, label = \"Validation Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()  "
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 951
        },
        "id": "2wgOz85ZLU3o",
        "outputId": "2004013a-4a39-408d-fc4e-c59b3bd7ccd1"
      },
      "source": [
        "#classification accuracy for different percentage of training samples\n",
        "for i in range(10):\n",
        "  train = df.sample(frac=((i+1)/10))\n",
        "  test = df.drop(train.index)\n",
        "  w=logistic_regression(train,0.2,0.001,5)\n",
        "  pred1=predict(train,w)\n",
        "  pred2=predict(test,w)\n",
        "  print(\"Train Accuracy for \",(i+1)*10,\"percentage of training samples: \",pred1[1])\n",
        "  print(\"Test Accuracy for \",(i+1)*10,\"percentage of training samples: \",pred2[1])\n",
        "  print(\"\\n\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy for  10 percentage of training samples:  50.0\n",
            "Test Accuracy for  10 percentage of training samples:  50.0\n",
            "\n",
            "\n",
            "Train Accuracy for  20 percentage of training samples:  30.0\n",
            "Test Accuracy for  20 percentage of training samples:  55.00000000000001\n",
            "\n",
            "\n",
            "Train Accuracy for  30 percentage of training samples:  53.333333333333336\n",
            "Test Accuracy for  30 percentage of training samples:  48.57142857142857\n",
            "\n",
            "\n",
            "Train Accuracy for  40 percentage of training samples:  50.0\n",
            "Test Accuracy for  40 percentage of training samples:  50.0\n",
            "\n",
            "\n",
            "Train Accuracy for  50 percentage of training samples:  44.0\n",
            "Test Accuracy for  50 percentage of training samples:  56.00000000000001\n",
            "\n",
            "\n",
            "Train Accuracy for  60 percentage of training samples:  48.333333333333336\n",
            "Test Accuracy for  60 percentage of training samples:  52.5\n",
            "\n",
            "\n",
            "Train Accuracy for  70 percentage of training samples:  50.0\n",
            "Test Accuracy for  70 percentage of training samples:  50.0\n",
            "\n",
            "\n",
            "Train Accuracy for  80 percentage of training samples:  52.5\n",
            "Test Accuracy for  80 percentage of training samples:  40.0\n",
            "\n",
            "\n",
            "Train Accuracy for  90 percentage of training samples:  52.22222222222223\n",
            "Test Accuracy for  90 percentage of training samples:  30.0\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ZeroDivisionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-e77059b4fc7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mpred1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   \u001b[0mpred2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Train Accuracy for \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"percentage of training samples: \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpred1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Test Accuracy for \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"percentage of training samples: \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpred2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-57b1fc119382>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(dataset, weights)\u001b[0m\n\u001b[1;32m     61\u001b[0m       \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mY_predict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_predict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
          ]
        }
      ]
    }
  ]
}